{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:34:46.067006Z",
     "iopub.status.busy": "2024-12-08T19:34:46.066563Z",
     "iopub.status.idle": "2024-12-08T19:34:57.873712Z",
     "shell.execute_reply": "2024-12-08T19:34:57.872869Z",
     "shell.execute_reply.started": "2024-12-08T19:34:46.066957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=file:///kaggle/input/bitsandbytes/ /kaggle/input/bitsandbytes/bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls gemma_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=file:///kaggle/input/libraries/transformer_peft_accelrator_bite/ peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls /kaggle/input/libraries/transformer_peft_accelrator_bite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=file:///kaggle/input/libraries/transformer_peft_accelrator_bite/ transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=file:///kaggle/input/libraries/transformer_peft_accelrator_bite/ tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=file:///kaggle/input/libraries/transformer_peft_accelrator_bite/ accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:47:47.311986Z",
     "iopub.status.busy": "2024-12-14T17:47:47.311007Z",
     "iopub.status.idle": "2024-12-14T17:48:01.017233Z",
     "shell.execute_reply": "2024-12-14T17:48:01.016362Z",
     "shell.execute_reply.started": "2024-12-14T17:47:47.311935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:15.005353Z",
     "iopub.status.busy": "2024-12-14T17:48:15.004714Z",
     "iopub.status.idle": "2024-12-14T17:48:25.652422Z",
     "shell.execute_reply": "2024-12-14T17:48:25.651580Z",
     "shell.execute_reply.started": "2024-12-14T17:48:15.005319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:25.654920Z",
     "iopub.status.busy": "2024-12-14T17:48:25.654346Z",
     "iopub.status.idle": "2024-12-14T17:48:25.660532Z",
     "shell.execute_reply": "2024-12-14T17:48:25.659704Z",
     "shell.execute_reply.started": "2024-12-14T17:48:25.654891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
    "\n",
    "# \"/kaggle/input/gemma-2-2b-it-unsloth-bnb-4bit-namm/transformers/default/1\"\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir =  \"/kaggle/input/gemma-2-2b-it-unsloth-bnb-4bit-namm/transformers/default/1\"\n",
    "    lora_dir = '/kaggle/input/checkpoint9000/output/checkpoint-8000'\n",
    "    max_length: int = 1024\n",
    "# Instantiate the Config class\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:25.661753Z",
     "iopub.status.busy": "2024-12-14T17:48:25.661486Z",
     "iopub.status.idle": "2024-12-14T17:48:27.236448Z",
     "shell.execute_reply": "2024-12-14T17:48:27.235492Z",
     "shell.execute_reply.started": "2024-12-14T17:48:25.661722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.gemma_dir)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:27.246358Z",
     "iopub.status.busy": "2024-12-14T17:48:27.246040Z",
     "iopub.status.idle": "2024-12-14T17:48:45.159256Z",
     "shell.execute_reply": "2024-12-14T17:48:45.158307Z",
     "shell.execute_reply.started": "2024-12-14T17:48:27.246322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    quantization_method=\"bnb\"\n",
    ")\n",
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.gemma_dir,\n",
    "    num_labels=2,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, config.lora_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.160776Z",
     "iopub.status.busy": "2024-12-14T17:48:45.160498Z",
     "iopub.status.idle": "2024-12-14T17:48:45.401223Z",
     "shell.execute_reply": "2024-12-14T17:48:45.400419Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.160750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the parquet file\n",
    "df = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\n",
    "\n",
    "# Save the dataframe as a CSV file with escape characters\n",
    "df.to_csv('/kaggle/working/test.csv', index=False, escapechar='\\\\')\n",
    "\n",
    "ds = Dataset.from_csv('/kaggle/working/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.402538Z",
     "iopub.status.busy": "2024-12-14T17:48:45.402291Z",
     "iopub.status.idle": "2024-12-14T17:48:45.410120Z",
     "shell.execute_reply": "2024-12-14T17:48:45.409336Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.402514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.411427Z",
     "iopub.status.busy": "2024-12-14T17:48:45.411095Z",
     "iopub.status.idle": "2024-12-14T17:48:45.421441Z",
     "shell.execute_reply": "2024-12-14T17:48:45.420522Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.411401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ds = ds.select(torch.arange(10000))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int = 1024) -> None:\n",
    "        \"\"\"\n",
    "        Custom tokenizer to process and tokenize prompt-response pairs with balanced token allocation.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (PreTrainedTokenizerBase): The tokenizer to use.\n",
    "            max_length (int): Maximum token length for the combined input.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_per_section = max_length // 3  # Allocate approximately 1/3 of max_length to each part\n",
    "        self.instruction = \"<instruction>: Compare two responses and decide which one answers the prompt better.\"\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes and processes a batch of data, ensuring balanced token distribution\n",
    "        across prompt, response_a, and response_b.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A dictionary containing \"prompt\", \"response_a\", \"response_b\", and \"winner\".\n",
    "\n",
    "        Returns:\n",
    "            dict: Processed batch with tokenized input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        processed_data = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "        \n",
    "        for i in range(len(batch[\"prompt\"])):\n",
    "            try:\n",
    "                # Process text parts\n",
    "                instruction = self.instruction\n",
    "                prompt = self.process_text(batch[\"prompt\"][i])\n",
    "                response_a = self.process_text(batch[\"response_a\"][i])\n",
    "                response_b = self.process_text(batch[\"response_b\"][i])\n",
    "                \n",
    "                # Combine instruction and text parts\n",
    "                combined_text = (\n",
    "                    instruction + \"\\n\\n\" +\n",
    "                    \"<prompt>: \" + prompt + \"\\n\\n\" +\n",
    "                    \"<response_a>: \" + response_a + \"\\n\\n\" +\n",
    "                    \"<response_b>: \" + response_b\n",
    "                )\n",
    "                \n",
    "                # Tokenize the combined text\n",
    "                tokenized = self.tokenizer(\n",
    "                    combined_text,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "                \n",
    "                # Handle the winner label\n",
    "                winner = batch[\"winner\"][i]\n",
    "                if winner == \"model_a\":\n",
    "                    label = 0\n",
    "                elif winner == \"model_b\":\n",
    "                    label = 1\n",
    "                else:\n",
    "                    continue  # Skip rows with invalid winner labels\n",
    "                \n",
    "                # Append processed data\n",
    "                processed_data[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "                processed_data[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "                processed_data[\"labels\"].append(label)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Log the error and continue (optional)\n",
    "                print(f\"Skipping row {i} due to error: {e}\")\n",
    "        \n",
    "        return processed_data\n",
    "\n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans and preprocesses text by removing null values and extra spaces.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        return text.replace(\"null\", \"\").strip()\n",
    "\n",
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.884961Z",
     "iopub.status.busy": "2024-12-14T17:48:45.884310Z",
     "iopub.status.idle": "2024-12-14T17:48:45.913161Z",
     "shell.execute_reply": "2024-12-14T17:48:45.912351Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.884919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds.save_to_disk('/kaggle/working/preprocessed_dataset')\n",
    "ds = Dataset.load_from_disk('/kaggle/working/preprocessed_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.914899Z",
     "iopub.status.busy": "2024-12-14T17:48:45.914287Z",
     "iopub.status.idle": "2024-12-14T17:48:45.919790Z",
     "shell.execute_reply": "2024-12-14T17:48:45.919012Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.914858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.921214Z",
     "iopub.status.busy": "2024-12-14T17:48:45.920891Z",
     "iopub.status.idle": "2024-12-14T17:48:45.947469Z",
     "shell.execute_reply": "2024-12-14T17:48:45.946880Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.921178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds['id'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'id': self.ds['id'][idx],\n",
    "            'input_ids': torch.tensor(self.ds['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.ds['attention_mask'][idx]),\n",
    "        }\n",
    "\n",
    "\n",
    "# THIS was causing bottleneck\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "# batch_size = 32  # Adjust based on your GPU memory\n",
    "# dataset = CustomDataset(ds)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4,shuffle=False,\n",
    "#                         persistent_workers=True,  pin_memory=torch.cuda.is_available\n",
    "# )\n",
    "# results = []\n",
    "\n",
    "# #No gradient computation\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "#         # Prepare batch inputs\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         example_ids = batch['id']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         predictions = torch.argmax(outputs['logits'], dim=1)  # Get batch predictions\n",
    "        \n",
    "#         # Map predictions to labels\n",
    "#         for example_id, prediction in zip(example_ids, predictions.cpu().numpy()):\n",
    "#             winner = \"model_a\" if prediction == 0 else \"model_b\"\n",
    "#             results.append({'id': example_id, 'winner': winner})\n",
    "\n",
    "# # Convert results to DataFrame and save as CSV\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:48:45.953926Z",
     "iopub.status.busy": "2024-12-14T17:48:45.953695Z",
     "iopub.status.idle": "2024-12-14T17:48:49.114597Z",
     "shell.execute_reply": "2024-12-14T17:48:49.113752Z",
     "shell.execute_reply.started": "2024-12-14T17:48:45.953904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  \n",
    "\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = torch.tensor(ds['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ds['attention_mask']).to(device)\n",
    "ids = ds['id']  \n",
    "\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for start_idx in tqdm(range(0, len(ids), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_input_ids = input_ids[start_idx:start_idx + batch_size]\n",
    "        batch_attention_mask = attention_mask[start_idx:start_idx + batch_size]\n",
    "        batch_ids = ids[start_idx:start_idx + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "\n",
    "        # Map predictions to labels\n",
    "        for example_id, prediction in zip(batch_ids, predictions.cpu().numpy()):\n",
    "            winner = \"model_a\" if prediction == 0 else \"model_b\"\n",
    "            results.append({'id': example_id, 'winner': winner})\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 5471921,
     "sourceId": 9071685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6242768,
     "sourceId": 10117964,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6243856,
     "sourceId": 10119385,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6244215,
     "sourceId": 10119829,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6303737,
     "sourceId": 10201030,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6316976,
     "sourceId": 10218950,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6321802,
     "sourceId": 10225560,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 124141,
     "modelInstanceId": 99970,
     "sourceId": 118886,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
